{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLANATION OF OUTPUT VARIABLES (the relevant trials are the \"canvas-keyboard-response\" ones):\n",
    "\n",
    "Experiment flow:\n",
    "- *block_index* -> index of the block (NB: starts from zero)\n",
    "- *presentation_index* -> index of the presentation inside one block (NB: starts from zero)\n",
    "\n",
    "Experiment parameters:\n",
    "- *clique_size* -> size of the clique in one of the two displayed graphs\n",
    "- *graphs_size* -> size of the displayed graphs \n",
    "- *graphs_couple* -> couple of graphs shown in current trial\n",
    "- *nodes_order* -> order of the nodes in current trial\n",
    "\n",
    "Dependent variables:\n",
    "- *rt* -> response time in ms (collected automatically)\n",
    "- *response* -> in this experiment, the accepted keys are \" \"; \"arrowright\" or \"arrowleft\"\n",
    "- *accuracy* -> this variable is manually computed and added only to the trials where response was arrowright or arrowleft\n",
    "\n",
    "Comparison variables:\n",
    "- *correct_response* -> correct response for current trial\n",
    "\n",
    "Produced automatically / not relevant:\n",
    "\n",
    "For all trials:\n",
    "- *trial_type* -> the name of the plugin used to run the trial (the real trials of the experiment are \"canvas-keyboard-response\")\n",
    "- *trial_index* -> the index of the current trial across the whole experiment (it is a global counter of trials, it also includes instructions and tutorial ones)\n",
    "- *time_elapsed* -> the number of milliseconds between the start of the experiment and when the trial ended\n",
    "- *internal_node_id* -> string identifier for the current TimelineNode\n",
    "\n",
    "For preload plugin:\n",
    "- *success*\t-> if true, then all files loaded successfully within the max_load_time. If false, then one or more file requests returned a failure and/or the file loading did not complete within the max_load_time duration.\n",
    "- *timeout* -> \tif true, then the files did not finish loading within the max_load_time duration. If false, then the file loading did not timeout.\n",
    "- *failed_images* -> one or more image file paths that produced a loading failure before the trial ended.\n",
    "- *failed_audio* -> one or more audio file paths that produced a loading failure before the trial ended.\n",
    "- *failed_video* -> one or more video file paths that produced a loading failure before the trial ended.\n",
    "\n",
    "For instructions plugin:\n",
    "- *rt* -> the response time (in ms) for the subject to view all of the pages\n",
    "- *view_history* -> array containing the order of pages the subject viewed (collected only for \"instructions\" plugin)\n",
    "\n",
    "For image-keyboard-response plugin:\n",
    "- *stimulus* -> the path of the image that was displayed.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# for swarmplot\n",
    "import seaborn as sns\n",
    "\n",
    "# # to correctly sort file names (maintaining correspondence with subject numbers):\n",
    "# from natsort import os_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and \"cleaning\" a single experiment csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465, 22)\n",
      "(180, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300.0    12\n",
       "267.0    12\n",
       "233.0    12\n",
       "217.0    12\n",
       "200.0    12\n",
       "183.0    12\n",
       "167.0    12\n",
       "150.0    12\n",
       "133.0    12\n",
       "117.0    12\n",
       "100.0    12\n",
       "83.0     12\n",
       "67.0     12\n",
       "50.0     12\n",
       "33.0     12\n",
       "Name: clique_size, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1Path = Path('./data/MOCKDATA_HUPLACLIP_pilot_26.6.2023_18.14.17.csv')\n",
    "df1 = pd.read_csv(file1Path)\n",
    "print(df1.shape)\n",
    "# isolating experiment trials (\"canvas-keyboard-response\" ones)\n",
    "df1_experiment = df1[df1.trial_type == \"canvas-keyboard-response\"]\n",
    "# dropping empty/irrelevant variables:\n",
    "df1_cleaned = df1_experiment.drop([\"view_history\",\"trial_index\", \"time_elapsed\",\"internal_node_id\" ,\"success\",\"stimulus\"], axis=1)\n",
    "# isolating trials were final responses were given (shuffles have \" \" as response):\n",
    "df1_cleaned_final_answers = df1_cleaned[df1_cleaned['response'].isin(['arrowright','arrowleft'])]\n",
    "print(df1_cleaned_final_answers.shape)     #final responses should be 180 (30 for each one of the 6 blocks)\n",
    "# counting the final answers for each clique size\n",
    "df1_cleaned_final_answers['clique_size'].value_counts()  #the clique sizes should be 15, and there should be 8 trials for each clique size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting fraction correct as a function of \"clique_size\"\n",
    "# - calculating accuracy for the different levels of \"clique_size\"\n",
    "fraction_correct_dict = dict()\n",
    "# reversing the order of clique_size array:\n",
    "reversed_clique_size_array = df1_cleaned_final_answers.clique_size.unique()[::-1]\n",
    "for i in reversed_clique_size_array:\n",
    "    # isolating data of current clique size:\n",
    "    current_data = df1_cleaned_final_answers[df1_cleaned_final_answers.clique_size == i]\n",
    "    # calculating fraction correct for current clique size\n",
    "    fraction_correct_current_clique_size = sum(current_data.accuracy == True) / sum(~ current_data.accuracy.isna())\n",
    "    # adding to the dictionary\n",
    "    fraction_correct_dict[str(round(i))] = fraction_correct_current_clique_size\n",
    "    \n",
    "    #CONTROL:\n",
    "    print(\"clique size:\")\n",
    "    print(i)\n",
    "    print(\"fraction correct:\")\n",
    "    print(fraction_correct_current_clique_size)\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "# plotting the results:\n",
    "# - obtaining appropriate x and y values:\n",
    "x_values = list(fraction_correct_dict.keys())\n",
    "x_values = [int(x_value)/(math.sqrt(300))  for x_value in x_values]\n",
    "y_values = list(fraction_correct_dict.values())\n",
    "y_values = [y_value * 100 for y_value in y_values]\n",
    "# - drawing plot:\n",
    "plt.plot(x_values,y_values)\n",
    "plt.xlabel(\"k/sqrt(300)\")\n",
    "plt.ylabel(\"% of correct responses\")\n",
    "# plotting chance level line and adding legend\n",
    "plt.axhline(y = 50, color = 'r', linestyle = 'dashed', label = \"chance level\")\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.suptitle(\"Accuracy as a function of K/sqrt(1000)\", fontsize = 15)\n",
    "plt.title(\"sample size = 1\", fontstyle= \"italic\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing multiple csv files, combining them in a single dataframe and removing irrelvant variables:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TO ADAPT FROM HERE ON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty dataframe:\n",
    "data = pd.DataFrame()\n",
    "# accessing all json files in 'data' directory:\n",
    "files = Path('./data/').glob('*.csv')\n",
    "fileCounter = 0\n",
    "for file in files:\n",
    "    # reading single csv file as dataframe\n",
    "    df = pd.read_csv(file)\n",
    "    # adding single dataframe to general dataframe:\n",
    "    data = pd.concat([data,df])\n",
    "    # incrementing file counter (will be used to label the graph):\n",
    "    fileCounter += 1\n",
    "\n",
    "# isolating experiment trials (\"canvas-keyboard-response\" ones)\n",
    "data_experiment = data[data.trial_type == \"canvas-keyboard-response\"]\n",
    "\n",
    "# dropping irrelevant variables:\n",
    "data_experiment.drop([\"view_history\",\"trial_index\", \"time_elapsed\",\"internal_node_id\" ,\"success\", \"timeout\", \"failed_images\", \"failed_audio\", \"failed_video\", \"stimulus\"], axis=1, inplace=True)\n",
    "\n",
    "# checking that cleaning happened successfully:\n",
    "print(data_experiment.columns.tolist())\n",
    "print(data_experiment.shape)\n",
    "\n",
    "# isolating trials were final responses were given (shuffles have \" \" as response):\n",
    "data_experiment_final = data_experiment[data_experiment['response'].isin(['arrowright','arrowleft'])]\n",
    "print(data_experiment_final.shape)     # final responses = 12*(number of csv files in data folder)\n",
    "# counting the final answers for each clique size\n",
    "print(data_experiment_final['clique_size'].value_counts()  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting fraction correct as a function of \"clique_size\"\n",
    "# - calculating accuracy for the different levels of \"clique_size\"\n",
    "fraction_correct_dict = dict()\n",
    "# reversing the order of clique_size array:\n",
    "reversed_clique_size_array = df1_cleaned_final_answers.clique_size.unique()[::-1]\n",
    "for i in reversed_clique_size_array:\n",
    "    # isolating data of current clique size:\n",
    "    current_data = data_experiment_final[data_experiment_final.clique_size == i]\n",
    "    # calculating fraction correct for current clique size\n",
    "    fraction_correct_current_clique_size = sum(current_data.accuracy == True) / sum(~ current_data.accuracy.isna())\n",
    "    # adding to the dictionary\n",
    "    fraction_correct_dict[str(round(i))] = fraction_correct_current_clique_size\n",
    "    \n",
    "    # # DEBUG:\n",
    "    # print(\"clique size:\")\n",
    "    # print(i)\n",
    "    # print(\"fraction correct:\")\n",
    "    # print(fraction_correct_current_clique_size)\n",
    "    # print(current_data.accuracy)\n",
    "    \n",
    "# plotting the results:\n",
    "# - obtaining appropriate x and y values:\n",
    "x_values = list(fraction_correct_dict.keys())\n",
    "x_values = [int(x_value)/(math.sqrt(300))  for x_value in x_values]\n",
    "y_values = list(fraction_correct_dict.values())\n",
    "y_values = [y_value * 100 for y_value in y_values]\n",
    "print(y_values)\n",
    "# - drawing plot:\n",
    "plt.plot(x_values,y_values)\n",
    "plt.xlabel(\"k/sqrt(300)\")\n",
    "plt.ylabel(\"% of correct responses\")\n",
    "# plotting chance level line and adding legend\n",
    "plt.axhline(y = 50, color = 'r', linestyle = 'dashed', label = \"chance level\")\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.suptitle(\"Accuracy as a function of K/sqrt(300)\", fontsize = 15)\n",
    "plt.title('sample size = {}'.format(fileCounter), fontstyle= \"italic\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separately calculating accuracy of the different subjects for the various levels of clique size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty dataframe:\n",
    "data = pd.DataFrame()\n",
    "# accessing all json files in 'data' directory:\n",
    "files = Path('./data/').glob('*.csv')\n",
    "# # reordering paths so that correspondence with subject number is maintained (otherwise subject 10 data is read first):\n",
    "# files = os_sorted(files)  NOT NEEDED BECAUSE NOW 01-02-03...\n",
    "# reversing the order of clique_size array (in experiment is 175 -> 0, in graph is 0 -> 175):\n",
    "reversed_clique_size_array = df1_cleaned_final_answers.clique_size.unique()[::-1]\n",
    "# subject counter (will be used to label the keys of the plot dictionary):\n",
    "subject_counter = 1\n",
    "# plot dataframe (rows = subjects ; columns = array of clique sizes. Each line contains the accuracy of that subject for all the clique sizes )\n",
    "plot_dict = dict()\n",
    "for file in files:\n",
    "    \n",
    "    # # DEBUG:\n",
    "    print(file)\n",
    "    \n",
    "    # reading single csv file as dataframe\n",
    "    df = pd.read_csv(file)\n",
    "    # isolating experiment trials:\n",
    "    df_experiment = df[df.trial_type == \"canvas-keyboard-response\"]\n",
    "    # dropping irrelevant variables:\n",
    "    df_experiment_cleaned = df_experiment.drop([\"view_history\",\"trial_index\", \"time_elapsed\",\"internal_node_id\" ,\"success\", \"timeout\", \"failed_images\", \"failed_audio\", \"failed_video\", \"stimulus\"], axis=1)\n",
    "    # isolating trials of final responses:\n",
    "    df_experiment_final = df_experiment_cleaned[df_experiment_cleaned['response'].isin(['arrowright','arrowleft'])]\n",
    "    # creating empty array of accuracies:\n",
    "    accuracies_for_subject = []\n",
    "    for i in reversed_clique_size_array:\n",
    "        # isolating data of current clique size:\n",
    "        current_data = df_experiment_final[df_experiment_final.clique_size == i]\n",
    "        # calculating fraction correct for current clique size and transforming it in percentage \n",
    "        fraction_correct_current_clique_size = ( sum(current_data.accuracy == True) / sum(~ current_data.accuracy.isna()) )*100\n",
    "        # appending single accuracy to the array of accuracies:\n",
    "        accuracies_for_subject.append(fraction_correct_current_clique_size)  \n",
    "    # adding accuracy array to the dictionary\n",
    "    plot_dict[subject_counter] = accuracies_for_subject\n",
    "    # increasing subject counter:\n",
    "    subject_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_dict)\n",
    "# printing separately the accuracies to check their correctness\n",
    "accuracies_per_subjects = list(plot_dict.values())\n",
    "accuracies_per_subjects = [print(subject_accuracies) for subject_accuracies in accuracies_per_subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframes of accuracies from dictionary (rows are subjects, columns are clique sizes):\n",
    "plot_dataframe = pd.DataFrame.from_dict(plot_dict, orient='index',columns=x_values)\n",
    "# printing dataframe:\n",
    "print(plot_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING DATAFRAME THAT SUITS SWARMPLOT:\n",
    "# - creating SUBJECT NUMBER column (1 (15 times), 2 (15 times)...):\n",
    "subject_number_list = sorted(list(range(1,11))*15)\n",
    "\n",
    "# - creating CLIQUE SIZE column (reversed_clique_size_array -> repeated 10 times)\n",
    "clique_size_list = []\n",
    "for i in range(10):\n",
    "    clique_size_list.extend(reversed_clique_size_array)\n",
    "\n",
    "# - creating X LABEL column (clique size transformed according to Rudy's graph):\n",
    "x_label_list = [round(int(x_value)/(math.sqrt(300)),1)  for x_value in reversed_clique_size_array]*10\n",
    "\n",
    "# - creating  SUBJECT ACCURACY column (long list of 150 values, 1:15 -> all mean accuracies for subj 1; 16:30 -> all mean accuracies for subj 2 ...)\n",
    "subject_accuracy_list = []\n",
    "for sublist in list(plot_dict.values()):\n",
    "    for item in sublist:\n",
    "        subject_accuracy_list.append(item)\n",
    "\n",
    "# creating dictionary for swarmplot:\n",
    "swarmplot_dict = { 'subject_number': subject_number_list, 'clique_size': clique_size_list, 'x_label': x_label_list, 'subject_accuracy': subject_accuracy_list}\n",
    "\n",
    "# creating dataframe from dictionary:\n",
    "swarmplot_df = pd.DataFrame.from_dict(swarmplot_dict)\n",
    "print(swarmplot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MEAN ACCURACY and put it in new dataframe:\n",
    "mean_accuracy_df = swarmplot_df.groupby('x_label')[\"subject_accuracy\"].mean().reset_index()\n",
    "print(mean_accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAT GPT CODE to generate graph:\n",
    "# Creating graph:\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# Set x and y axis limits:\n",
    "ax.set_xlim(0, 11)\n",
    "ax.set_ylim(0, 105)\n",
    "# Swarmplot\n",
    "swarm_plot = sns.swarmplot(data=swarmplot_df, x='x_label', y='subject_accuracy', palette='tab20', hue='subject_number', size=4, ax=ax)\n",
    "# Scatterplot\n",
    "mean_df_grouped = mean_accuracy_df.groupby('x_label').mean().reset_index()\n",
    "for i, row in mean_df_grouped.iterrows():\n",
    "    x = row['x_label']\n",
    "    y = row['subject_accuracy']\n",
    "    # Get the x-coordinate of the middle of the swarmplot group\n",
    "    swarm_x = swarm_plot.collections[i].get_offsets()[:, 0].mean()\n",
    "    # Plot the scatterplot marker at the swarm_x coordinate\n",
    "    sns.scatterplot(x=[swarm_x], y=[y], color='blue', s=80, marker='X', ax=ax)\n",
    "\n",
    "# Add vertical lines at each x tick\n",
    "x_ticks = ax.get_xticks()\n",
    "for x_tick in x_ticks:\n",
    "    ax.axvline(x=x_tick, color='gray', linewidth=0.5)\n",
    "\n",
    "# Move swarmplot legend to bottom-right\n",
    "handles, labels = swarm_plot.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[1:], labels=labels[1:], bbox_to_anchor=(1, 0), loc='lower right')\n",
    "\n",
    "# Add custom legend for blue marker\n",
    "ax.plot([], [], color='blue', marker='X', label=\"Average\", linestyle='None')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Personalizing graph:\n",
    "plt.axhline(50, color=\"red\", linestyle=\"dashdot\", linewidth=0.3)\n",
    "plt.annotate('chance level', xy=(8, 45), xytext=(8, 45), color=\"red\", fontsize=10)\n",
    "ax.set_xlabel('k/sqrt(300)')\n",
    "ax.set_ylabel('% of correct responses')\n",
    "plt.title(\"Subjects' accuracy as a function of K/sqrt(300)\", fontsize=15)\n",
    "# Visualizing graph:\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIPPLOT:\n",
    "graph=sns.stripplot(data=swarmplot_df,  x=\"x_label\", y=\"subject_accuracy\", hue= \"subject_number\", size = 3)\n",
    "graph.axhline(0.5, color = \"red\", linestyle = \"dashdot\", linewidth=0.3)\n",
    "graph.annotate('chance level', xy=(4, 0.45), xytext=(4, 0.45) )\n",
    "graph.set_title(\"Accuracy as a function of K/sqrt(300)\", fontsize = 15)\n",
    "graph.set(xlabel='k/sqrt(300)', ylabel=\"% of correct responses\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating time needed for each block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing a single file and calculating average time for each block:\n",
    "filePath = Path('./data/HUPLACLIP_pilot_subj10_2023-04-27_18h19.29.962.csv')\n",
    "df = pd.read_csv(filePath)\n",
    "# isolating experiment trials (\"canvas-keyboard-response\" ones)\n",
    "df_experiment = df[df.trial_type == \"canvas-keyboard-response\"]\n",
    "# dropping empty/irrelevant variables (only \"time_elapsed\" is used to calculate the time):\n",
    "df_experiment = df_experiment.drop([\"view_history\",\"trial_index\",\"internal_node_id\" ,\"success\", \"timeout\", \"failed_images\", \"failed_audio\", \"failed_video\", \"stimulus\", \"nodes_order\",\"response\",\"correct_response\",\"graphs_couple\",\"accuracy\",\"presentation_index\",\"clique_size\",\"graph_size\"], axis=1)\n",
    "# for each of the 4 blocks, extracting the first and the last trial and caculating the difference in time:\n",
    "blocks_duration_list = []\n",
    "for i in range(4):\n",
    "    # isolating trials for current block\n",
    "    df_current_block = df_experiment[df_experiment[\"block_index\"] == i]\n",
    "    # calculating the difference in time_elapsed between first and last element, transforming it in minutes\n",
    "    blocks_duration_list.append(( df_current_block.tail(1).time_elapsed.values[0] - df_current_block.head(1).time_elapsed.values[0]) * 1.6667*10**(-5))\n",
    "\n",
    "# printing the list:\n",
    "print(blocks_duration_list)\n",
    "# calculating average of the list:\n",
    "print(sum(blocks_duration_list) / len(blocks_duration_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TO ADAPT FROM HERE ON)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. overall accuracy ( # correct answers / # final answers )\n",
    "overall_accuracy = sum(data_experiment.correctnessOfResponse == 1) / sum(~ data_experiment.correctnessOfResponse.isna())\n",
    "print(overall_accuracy)\n",
    "\n",
    "# 2. calculating accuracy for the levels of \"cliqueSize\"\n",
    "# - obtaining levels of \"cliqueSize\" (this should match with experiment parameters)\n",
    "print(data_experiment.cliqueSize.unique())\n",
    "# - calculating accuracy for the different levels of \"cliqueSize\"\n",
    "accuracy_dict = dict()\n",
    "for i in data_experiment.cliqueSize.unique():\n",
    "    # isolating data of current clique size:\n",
    "    current_data = data_experiment[data_experiment.cliqueSize == i]\n",
    "    accuracy_current_cliqueSize = sum(current_data.correctnessOfResponse == 1) / sum(~ current_data.correctnessOfResponse.isna())\n",
    "    accuracy_dict[str(round(i))] = accuracy_current_cliqueSize\n",
    "# plotting the results:\n",
    "x_values = accuracy_dict.keys()\n",
    "y_values = accuracy_dict.values()\n",
    "plt.plot(x_values,y_values)\n",
    "plt.xlabel(\"clique size in graph of 300 nodes\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.tick_params(axis='x', which='major', labelsize=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of shuffles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. average number of shuffles\n",
    "# 2. calculating number of shuffles for the levels of \"cliqueSize\"  (plotting the results)\n",
    "# note: if number of shuffles increases as a function of \"cliqueSize\", it could be helping subjects for difficult instances of the task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reaction times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. average RT (for all trials, both final responses and shuffles)\n",
    "# 2. RT as a function of \"cliqueSize\"\n",
    "# note: if reaction time increases as a function of \"cliqueSize\", it could be an indication of increasing difficulty\n",
    "\n",
    "###########################################################\n",
    "\n",
    "\n",
    "# histogram of reaction times distribution (TO REVIEW):\n",
    "data.rt.hist(bins=100)\n",
    "# counting trials that were too slow and removing them (exclusion criterion: rt>7500)\n",
    "print((data.rt>=7500).value_counts())\n",
    "data_filtered = data.query('rt < 7500')\n",
    "\n",
    "#plotting unfiltered and filtered data:\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2,sharex=True)\n",
    "min_ylim, max_ylim = plt.ylim()\n",
    "#unfiltered data\n",
    "ax1.hist(data.rt,bins=100)\n",
    "ax1.axvline(data.rt.mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "ax1.text(data.rt.mean()*1.1,max_ylim*0.9, 'Mean: {:.2f}'.format(data.rt.mean()))\n",
    "ax1.set_title(\"RTs of Unfiltered data\")\n",
    "#filtered data\n",
    "ax2.hist(data_filtered.rt,bins=100)\n",
    "ax2.axvline(data_filtered.rt.mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "ax2.text(data_filtered.rt.mean()*1.1,max_ylim*0.9, 'Mean: {:.2f}'.format(data_filtered.rt.mean()))\n",
    "ax2.set_title(\"RTs of Filtered data\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd949ed4b94748d1150ad3fc7911325c390d4bc9b8bde526abb5c33ea40c1078"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
